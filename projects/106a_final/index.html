<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Handshake Bot | Naman Satish </title> <meta name="author" content="Naman Shimoga Satish"> <meta name="description" content="Final Project in C106A : Introduction to Robotics"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%90&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://satish.dev/projects/106a_final/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(51,48,215,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}#inspirational_quote{font-style:italic;color:#555;text-align:center;margin:20px 0}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Handshake Bot",
            "description": "Final Project in C106A : Introduction to Robotics",
            "published": "December 18, 2024",
            "authors": [
              
              {
                "author": "Alexander Lui",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "UC Berkeley",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Evan Chang",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "UC Berkeley",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Naman Satish",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "UC Berkeley",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Saurav Suresh",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "UC Berkeley",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Naman Satish </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Handshake Bot</h1> <p>Final Project in C106A : Introduction to Robotics</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#design">Design</a> </div> <div> <a href="#implementation">Implementation</a> </div> <div> <a href="#results">Results</a> </div> <div> <a href="#conclusion">Conclusion</a> </div> </nav> </d-contents> <h1 id="introduction">Introduction</h1> <h2 id="goal">Goal</h2> <p>Our goal was to create a robot that could perform a handshake with a human.</p> <div class="row mx-auto" style="text-align: center;"> <div class="col-sm mt-3 mt-md-0" id="inspirational_quote"> “humanity lies in the details – a truly human-like robot needs to do the very human task of giving a handshake” - Team 38 </div> </div> <p>Robots may appear inherently inhuman, with mechanical components designed to move through space with precision and speed, therefore they lack the organic movements normal to humans. Our project aimed to bridge this gap by creating a robot that could perform a handshake with a human, turning robotic motion into something organic and human-like. This task is deceptively complex, as it requires a robot to move through it’s joint space in a way that is both precise and fluid, while also being able to sense the presence of a human hand and adjust its motion accordingly.</p> <h2 id="challenges">Challenges</h2> <ol> <li> <strong>Computer Vision Perception</strong>: The robot must be able to continously sense the presence of a human hand in its workspace, and track the movement of the hand and extract its 3D position in the robot’s coordinate frame. This requires a robust computer vision system that can handle occlusions, lighting changes, and other environmental factors, and must be quick enough to provide real-time tracking of the hand.</li> <li> <strong>Path Planning and Motion</strong>: The robot must be able to move its end effector to a desired position in space, whilst also matching the movement of the hand it is tracking. This cannot consist of rotations that would be unnatural in a handshake. Therefore, the robot cannot use normal IK solutions as that would result in unnatural movements, and would not result in a human-like handshake.</li> </ol> <h1 id="design">Design</h1> <h2 id="design-criteria-and-functional-requirements">Design Criteria and Functional Requirements</h2> <ol> <li>We wanted to utilize a camera and CV system to visually track the human hand and determine the location that the robot should move to based on a specific handshake trajectory.</li> <li>The robot should perform movement within a designated area and should not harm the human, surrounding objects, or itself.</li> <li>The robot should be able to match or mirror the movement of the human hand in order to perform a handshake.</li> <li>The robot should execute paths in a smooth manner such that the movement looks like a natural handshake (i.e. no unnecessary twists and rotations of joints that look unnatural)</li> </ol> <h2 id="system-architecture">System Architecture</h2> <p>We decided to go with a 4 Module System Architecture:</p> <ol> <li> <strong>Hand Tracking</strong>: This module contained our hand tracking algorithm which relied on the MediaPipe’s ML model to track the hand in 2D space, and then used the depth information from the RealSense camera to extract the 3D position of the hand in the camera’s coordinate frame.</li> <li> <strong>Hand Transformation</strong>: This module transformed the 3D position of the hand from the camera’s coordinate frame to the robot’s coordinate frame. This was done using predefined transformation parameters that were calculated using QR codes.</li> <li> <strong>Handshake Procedure / Mirroring</strong>: This module was responsible for calculating the desired position of the robot’s end effector based on the 3D position of the hand. By reading a predefined trajectory, the published position of the robot’s end effector could be updated throughout the handshake procedure.</li> <li> <strong>Path Execution</strong>: This module was responsible for executing the path calculated by the Handshake Procedure module. It construct trajectories based on a collection of waypoints and then executed the trajectory using the MoveIt! library.</li> </ol> <h2 id="design-choices-and-trade-offs">Design Choices and Trade-offs</h2> <h3 id="camera-setup">Camera Setup</h3> <p>We opted for a single RealSense camera with depth information mounted on a tripod to have a clear view of the scene. This setup allowed us to use a single camera system while still being able to extract 3D coordinates from the camera data. However, we had to position the tripod at a sufficiently large distance to ensure there was enough room for the robot arm to move without obscuring the human’s hand. We also attached an AR tag to the camera lens to determine the camera’s position from the robot’s frame. This setup had some issues, such as inconsistent positioning and uncertainty about the orientation of the camera’s axes, which affected the accuracy of the position.</p> <h3 id="hand-tracking">Hand Tracking</h3> <p>We chose to track the feature point of the wrist, as it provided a consistent point for tracking the human’s motion. If we had more time, we could have used additional points to determine the orientation of the hand as well, however because some of the points on the hand are obscured, we could not have simply used the depth map to compute the 3D points, and would have had to use percieved depth information from Mediapipe in combination with the real depth at a certain point to compute 3D coordinates of 3 points on the hand. We streamed the data of the points at the frame rate of the camera, as we believed having more data would be generally helpful for the trajectory and we could figure out how to use it later.</p> <h3 id="handshake-procedure">Handshake Procedure</h3> <p>For simplicity, we mirrored the hand’s position across the x, y, and z planes, which allowed us to achieve almost all the functionality we desired for executing a handshake procedure. We did not publish orientation information due to time constraints and focused more on achieving smooth motion.</p> <h3 id="path-execution">Path Execution</h3> <p>Initially, we tested simple linear path following, but it was too restrictive for the points we were feeding in, causing the robot to not move. We then tested simple approaches using the MoveIt controller to reach the published points, but this resulted in strange paths and jittery movement as the robot would stop after each path was executed. To address this, we set simple thresholds on the distance between points and adjusted the sampling rate by taking the median of a window of 5 points to remove outliers and reduce unnecessary movements. We also set joint constraints on the Sawyer arm to prevent unnecessary joint rotations and created bounding box objects to prevent collisions with obstacles. Finally, we used Cartesian path planning to take in a queue of points, which led to some latency in determining the path but resulted in smoother and more consistent movement.</p> <h3 id="impact-on-real-engineering-applications">Impact on Real Engineering Applications</h3> <p>These design choices impacted the project’s robustness, durability, and efficiency. The single camera setup provided a cost-effective solution but introduced challenges in positioning and orientation accuracy. The wrist tracking method ensured consistent motion tracking but limited the ability to capture hand orientation. The simplified mirroring approach allowed for functional handshakes but lacked orientation data, which could be critical in more complex applications. The path execution improvements, such as joint constraints and Cartesian path planning, enhanced the smoothness and consistency of the robot’s movements, making the system more reliable and efficient in real-world scenarios. However without online path planning, the robot’s movements were slower and less responsive, meaning it could not keep up with the human’s movements.</p> <h1 id="implementation">Implementation</h1> <h2 id="hardware">Hardware</h2> <p>We used the following hardware components for our project:</p> <ol> <li> <p><strong>Sawyer Robot by Rethink Robotics</strong>: A versatile and collaborative robot arm. <a href="https://robotsguide.com/robots/sawyer" rel="external nofollow noopener" target="_blank">Sawyer Robot by Rethink Robotics</a></p> </li> <li> <p><strong>Intel RealSense Depth Camera D435</strong>: A depth camera used for tracking the human hand in 3D space. <a href="https://www.intelrealsense.com/depth-camera-d435/" rel="external nofollow noopener" target="_blank">Intel RealSense Depth Camera D435</a></p> </li> </ol> <div class="row mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ee106a/robot_and_camera-480.webp 480w,/assets/img/ee106a/robot_and_camera-800.webp 800w,/assets/img/ee106a/robot_and_camera-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ee106a/robot_and_camera.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="parts-used">Parts Used</h2> <ul> <li> <strong>Sawyer Robot</strong>: For executing the handshake motion.</li> <li> <strong>Intel RealSense Depth Camera D435</strong>: For capturing the 3D position of the human hand.</li> <li> <strong>Tripod</strong>: To mount the RealSense camera.</li> <li> <strong>AR Tag</strong>: For calibrating the camera’s position relative to the robot.</li> </ul> <h2 id="software">Software</h2> <p>We began our implementation from Lab 5 and Lab 7 codebases which provided us with functionality for inverse kinematics and QR code detection.</p> <p>The software components we developed include:</p> <ol> <li> <p><strong>cam_transform.py</strong>:</p> <ul> <li>Creates a TF Buffer to look up the transform between the robot base frame and the AR marker.</li> <li>Saves this transform to a JSON file.</li> <li>Provides a function for loading a transform from a JSON file.</li> </ul> </li> <li> <p><strong>realsense_tracking.py</strong>:</p> <ul> <li>Creates a <code class="language-plaintext highlighter-rouge">hand_pose_tracking</code> node and <code class="language-plaintext highlighter-rouge">/hand_pose</code> publisher that publishes a <code class="language-plaintext highlighter-rouge">PointStamped</code> object (queue size of 1).</li> <li>Starts up the RealSense pipeline for streaming images from the RealSense camera.</li> <li>Creates a <code class="language-plaintext highlighter-rouge">HandLandmarker</code> MediaPipe object and processes images to find the coordinates of hand landmarks, specifically the wrist point.</li> <li>Converts the wrist point into a 3D coordinate and publishes it to the <code class="language-plaintext highlighter-rouge">/hand_pose</code> topic.</li> </ul> </li> <li> <p><strong>realtime_move.py</strong>:</p> <ul> <li>Creates an <code class="language-plaintext highlighter-rouge">arm_controller</code> node that subscribes to <code class="language-plaintext highlighter-rouge">/hand_pose</code>.</li> <li>Loads the transform calculated by <code class="language-plaintext highlighter-rouge">cam_transform.py</code>.</li> <li>Broadcasts the frame of the camera location for visualization purposes.</li> <li>Performs the transformation from the camera frame to the robot frame of the <code class="language-plaintext highlighter-rouge">PointStamped</code> read from <code class="language-plaintext highlighter-rouge">/hand_pose</code> and publishes it to <code class="language-plaintext highlighter-rouge">/hand_pose_base</code>.</li> </ul> </li> <li> <p><strong>handshake_procedure.py</strong>:</p> <ul> <li>Creates a <code class="language-plaintext highlighter-rouge">handshake_procedure</code> node.</li> <li>Creates a subscriber to <code class="language-plaintext highlighter-rouge">hand_pose_base</code>.</li> <li>Takes in a command line argument of interactive or the JSON file with procedure configurations (interactive asks for user input of which axes to mirror about and duration of the procedure, creates <code class="language-plaintext highlighter-rouge">ProcedureStep</code> object and saves to a JSON file).</li> <li> <code class="language-plaintext highlighter-rouge">ProcedureStep</code>: boolean <code class="language-plaintext highlighter-rouge">axis_x</code>, boolean <code class="language-plaintext highlighter-rouge">axis_y</code>, boolean <code class="language-plaintext highlighter-rouge">axis_z</code>, float <code class="language-plaintext highlighter-rouge">duration</code>.</li> <li> <code class="language-plaintext highlighter-rouge">HandshakeProcedure</code>: loops for the duration amount of time and continuously calculates and publishes the desired robot point as a <code class="language-plaintext highlighter-rouge">PointStamped</code> to the <code class="language-plaintext highlighter-rouge">/robot_point</code> publisher.</li> </ul> </li> <li> <p><strong>move_hand.py</strong>:</p> <ul> <li>Creates an <code class="language-plaintext highlighter-rouge">arm_controller</code> node.</li> <li>Creates a <code class="language-plaintext highlighter-rouge">MoveGroupCommander</code> using the RRT planner.</li> <li>Creates <code class="language-plaintext highlighter-rouge">CollisionObject</code>: creates the collision box objects representing the walls and ceiling of the arm environment.</li> <li>Specifies <code class="language-plaintext highlighter-rouge">JointConstraint</code> objects and adds them as constraints to the planner.</li> <li>Creates a subscriber to <code class="language-plaintext highlighter-rouge">/robot_point</code> and runs <code class="language-plaintext highlighter-rouge">pose_callback()</code>.</li> <li> <code class="language-plaintext highlighter-rouge">pose_callback</code>: Reads in the desired point and adds the point to the waypoints queue if it meets the specified threshold for L2 distance away.</li> <li>Attempts to create a plan using <code class="language-plaintext highlighter-rouge">compute_cartesian_path</code> with the queue of waypoints (avoid_collision=True), and executes this plan.</li> </ul> </li> </ol> <div class="row mx-auto"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ee106a/rqt_graph-480.webp 480w,/assets/img/ee106a/rqt_graph-800.webp 800w,/assets/img/ee106a/rqt_graph-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ee106a/rqt_graph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h2 id="system-workflow">System Workflow</h2> <ol> <li> <strong>Start Up</strong>: Start the AR tracking packages and joint action server.</li> <li> <strong>Calibrate Camera</strong>: Move the RealSense camera with the attached AR tag into the frame of the right arm camera. Run <code class="language-plaintext highlighter-rouge">cam_transform.py</code> to determine the transform from the camera frame to the robot frame and save the transform as a JSON file.</li> <li> <strong>Hand Tracking</strong>: Remove the AR tag and run <code class="language-plaintext highlighter-rouge">realsense_tracking.py</code> to start an interactive window displaying the hand features being tracked and published to the <code class="language-plaintext highlighter-rouge">/hand_pose</code> topic. Additionally run <code class="language-plaintext highlighter-rouge">realtime_move.py</code> to transform the hand pose from the camera frame to the robot frame and publish it to the <code class="language-plaintext highlighter-rouge">/hand_pose_base</code> topic.</li> <li> <strong>Handshake Procedure</strong>: Run <code class="language-plaintext highlighter-rouge">handshake_procedure.py</code> and specify the desired handshake configuration (i.e., which axes to mirror about). This procedure is saved to a JSON file, which can be specified as a command line argument to skip this step in the future. Center the hand in the window and establish the point to mirror about.</li> <li> <strong>Path Execution</strong>: Run <code class="language-plaintext highlighter-rouge">move_hand.py</code> to start the path planner and movement node. The Sawyer arm will start following/mirroring hand movements.</li> </ol> <h1 id="results">Results</h1> <p>Our project was able to successfully track a human hand in 3D space and mirror its movements with the Sawyer robot. The robot was able to perform a handshake with a human, moving its end effector in a smooth and natural manner. It was both capable of tracking human hand movements and mirroring them in order to create a handshake motion. The robot was able to execute the handshake procedure within a designated area and did not harm the human, surrounding objects, or itself.</p> <div class="row l-page mx-auto"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ee106a/working_handshake-480.webp 480w,/assets/img/ee106a/working_handshake-800.webp 800w,/assets/img/ee106a/working_handshake-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ee106a/working_handshake.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ee106a/robot_mirroring-480.webp 480w,/assets/img/ee106a/robot_mirroring-800.webp 800w,/assets/img/ee106a/robot_mirroring-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ee106a/robot_mirroring.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h1 id="conclusion">Conclusion</h1> <h3 id="results-1">Results</h3> <p>Our results largely matched our design criteria, except for some minor inaccuracies and latency. As shown in the video, we were able to make the Sawyer robot mirror and track in such a way that it could perform certain kinds of handshakes with a human partner. The movement was somewhat slow due to the time taken to perform real-time path planning. The tracking could also be slightly inaccurate due to human error during camera calibration or if the camera was moved.</p> <h3 id="difficulties">Difficulties</h3> <p>We encountered difficulties calibrating the transform from the camera frame to the robot base frame. This was caused by the poor quality of the robot wrist camera, bad lighting, glare from the camera, and interference from other AR markers. We overcame these problems by moving the wrist camera closer, turning off the RealSense camera, and recalibrating until the transform stabilized. We also had issues with the path planning settling on paths that were unnecessarily long or dangerous. We solved this by adding collision boxes to bound the arm’s workspace and by adding joint constraints. Additionally, we used a Cartesian path planner, which planned a path based on a set of waypoints, producing more sensible paths. We also encountered a delay in the robot’s movement due to the time spent on path planning.</p> <h3 id="flaws-and-hacks">Flaws and Hacks</h3> <p>Our method for transforming hand points in the camera frame to the robot base frame is somewhat improvised. Ideally, we would streamline a process where the Sawyer robot automatically computes this transform without the need for an AR marker and manual calibration. To reduce latency caused by path planning, we filtered out points that were close together to lessen the load on the path planner. While effective to some extent, a more robust solution would involve optimizing the path planning process itself to enhance speed. Even with filtering, delays persist, requiring the human partner to move slowly for successful tracking and mirroring. With additional time, we would focus on speeding up path planning and improving the responsiveness of the robot.</p> <h1 id="team">Team</h1> <div class="row mx-auto"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ee106a/team_photo-480.webp 480w,/assets/img/ee106a/team_photo-800.webp 800w,/assets/img/ee106a/team_photo-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/ee106a/team_photo.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="members">Members</h2> <ol> <li>Saurav Suresh: Senior majoing in Computer Science and Math.</li> <li>Naman Satish: Senior majoring in Electrical Engineering and Computer Science, with a focus in Computer Vision.</li> <li>Alexander Lui: Senior majoring in Computer Science.</li> <li>Evan Chang: Senior majoring in Electrical Engineering and Computer Science.</li> </ol> <h2 id="contributions">Contributions</h2> <ol> <li>Saurav Suresh: Realsense Tracking</li> <li>Naman Satish: Handshake Procedure</li> <li>Alexander Lui : Move Hand</li> <li>Evan Chang: Cam Transform</li> </ol> <h1 id="additional-materials">Additional Materials</h1> <h1 id="code">Code</h1> <p><a href="https://github.com/NamanSatish/HandshakeBot" rel="external nofollow noopener" target="_blank">Handshake Bot Github</a></p> <p><a href="https://drive.google.com/file/d/1uUCDI7Fie_2VFIInZSgS9vOE47zhxm4-/view?usp=drive_link" rel="external nofollow noopener" target="_blank">Software Demo Video</a></p> <p><a href="https://drive.google.com/file/d/1KpU2-hxeUGlkkliAK7MLH2-zhfnthcEd/view?usp=drive_link" rel="external nofollow noopener" target="_blank">Handshake Video</a></p> <p><a href="https://docs.google.com/presentation/d/1GoCEK2VPXtKTkxlpo6xZE2zSntcXuxX6zAFLYc7gf7s/edit?usp=sharing" rel="external nofollow noopener" target="_blank">Project Presentation</a></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/empty.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Naman Shimoga Satish. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"Naman Satish's CV",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-computational-photography",title:"Computational Photography",description:"Experimenting with computational photography on vacation!",section:"Posts",handler:()=>{window.location.href="/blog/2024/computational-photography/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-handshake-bot",title:"Handshake Bot",description:"Final Project in C106A : Introduction to Robotics",section:"Projects",handler:()=>{window.location.href="/projects/106a_final/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"projects-rasterizer",title:"Rasterizer",description:"Part of CS184 : Foundations of Computer Graphics",section:"Projects",handler:()=>{window.location.href="/projects/homework_1/"}},{id:"projects-bezier-curves-and-triangle-meshes",title:"Bezier Curves and Triangle Meshes",description:"Part of CS184 : Foundations of Computer Graphics",section:"Projects",handler:()=>{window.location.href="/projects/homework_2/"}},{id:"projects-path-tracing",title:"Path Tracing",description:"Part of CS184 : Foundations of Computer Graphics",section:"Projects",handler:()=>{window.location.href="/projects/homework_3/"}},{id:"projects-prokudin-gorskii-photo-collection",title:"Prokudin-Gorskii Photo Collection",description:"Part of CS180 : Intro to Computer Vision and Computational Photography",section:"Projects",handler:()=>{window.location.href="/projects/proj_1/"}},{id:"projects-filters-amp-frequencies",title:"Filters & Frequencies",description:"Part of CS180 : Intro to Computer Vision and Computational Photography",section:"Projects",handler:()=>{window.location.href="/projects/proj_2/"}},{id:"projects-face-morphing",title:"Face Morphing",description:"Part of CS180 : Intro to Computer Vision and Computational Photography",section:"Projects",handler:()=>{window.location.href="/projects/proj_3/"}},{id:"projects-auto-stitching-and-photo-mosaics",title:"(Auto)Stitching and Photo Mosaics",description:"Part of CS180 : Intro to Computer Vision and Computational Photography",section:"Projects",handler:()=>{window.location.href="/projects/proj_4/"}},{id:"projects-diffusion-models",title:"Diffusion Models",description:"Part of CS180 : Intro to Computer Vision and Computational Photography",section:"Projects",handler:()=>{window.location.href="/projects/proj_5/"}},{id:"projects-neural-radiance-fields",title:"Neural Radiance Fields",description:"Final Project in CS180 : Intro to Computer Vision and Computational Photography",section:"Projects",handler:()=>{window.location.href="/projects/proj_final/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%6E%61%6D%61%6E@%62%65%72%6B%65%6C%65%79.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>